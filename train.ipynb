{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HoDa7tGE_ZF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extremely Minimalistic Implementation of DDPM\n",
        "https://arxiv.org/abs/2006.11239\n",
        "Everything is self contained. (Except for pytorch and torchvision... of course)\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFduuOpdkVpK"
      },
      "outputs": [],
      "source": [
        "def ddpm_schedules(beta1: float, beta2: float, T: int) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Returns pre-computed schedules for DDPM sampling, training process.\n",
        "    \"\"\"\n",
        "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
        "\n",
        "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
        "    sqrt_beta_t = torch.sqrt(beta_t)\n",
        "    alpha_t = 1 - beta_t\n",
        "    alphabar_t = torch.cumprod(alpha_t, dim=0)\n",
        "\n",
        "    return {\n",
        "        \"alpha_t\": alpha_t,\n",
        "        \"sqrt_beta_t\": sqrt_beta_t,\n",
        "        \"alphabar_t\": alphabar_t,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7TR3HSi2iZr",
        "outputId": "6f244c31-3536-4701-e1c4-4ea764716563"
      },
      "outputs": [],
      "source": [
        "print(ddpm_schedules(beta1=1e-4, beta2=0.02, T=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho4sNEygEzPA"
      },
      "outputs": [],
      "source": [
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        autoencoder_model: nn.Module,\n",
        "        betas: Tuple[float, float],\n",
        "        n_T: int,\n",
        "        criterion: nn.Module = nn.MSELoss(),\n",
        "    ) -> None:\n",
        "        super(DDPM, self).__init__()\n",
        "        self.autoencoder_model = autoencoder_model\n",
        "\n",
        "        # register_buffer allows us to freely access these tensors by name. It helps device placement.\n",
        "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
        "            self.register_buffer(k, v)\n",
        "\n",
        "        self.n_T = n_T\n",
        "        self.criterion = criterion\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Implements Algorithm 1 from the paper.\n",
        "        Makes forward diffusion x_t, and tries to guess epsilon value from x_t using autoencoder_model.\n",
        "        \"\"\"\n",
        "\n",
        "        t = torch.randint(1, self.n_T, (x.shape[0],)).to(x.device)  # t ~ Uniform(0, n_T)\n",
        "        eps = torch.randn_like(x)  # eps ~ N(0, 1)\n",
        "\n",
        "        x_t = (\n",
        "            torch.sqrt(self.alphabar_t[t, None, None, None]) * x\n",
        "            + torch.sqrt(1 - self.alphabar_t[t, None, None, None]) * eps\n",
        "        )\n",
        "\n",
        "        return self.criterion(eps, self.autoencoder_model(x_t))#, t / self.n_T))\n",
        "\n",
        "    def sample(self, n_sample: int, size, device) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Implements Algorithm 2 from the paper.\n",
        "        \"\"\"\n",
        "\n",
        "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1)\n",
        "\n",
        "        for i in range(self.n_T, 0, -1):\n",
        "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
        "            eps = self.autoencoder_model(x_i) #, i / self.n_T)\n",
        "            x_i = (\n",
        "                (1 / torch.sqrt(self.alpha_t[i])) *\n",
        "                (x_i - eps * (1 - self.alpha_t[i]) / torch.sqrt(1 - self.alphabar_t[i]))\n",
        "                + self.sqrt_beta_t[i] * z\n",
        "            )\n",
        "\n",
        "        return x_i\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueofC9Q4ag_9"
      },
      "outputs": [],
      "source": [
        "blk = lambda ic, oc: nn.Sequential(\n",
        "    nn.Conv2d(ic, oc, 7, padding=3),\n",
        "    nn.BatchNorm2d(oc),\n",
        "    nn.LeakyReLU(),\n",
        ")\n",
        "\n",
        "class AutoEncoderModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This should be unet-like, but let's don't think about the model too much :P\n",
        "    Basically, any universal R^n -> R^n model should work.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channel: int) -> None:\n",
        "        super(AutoEncoderModel, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            blk(n_channel, 64),\n",
        "            blk(64, 128),\n",
        "            blk(128, 256),\n",
        "            blk(256, 512),\n",
        "            blk(512, 256),\n",
        "            blk(256, 128),\n",
        "            blk(128, 64),\n",
        "            nn.Conv2d(64, n_channel, 3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t) -> torch.Tensor:\n",
        "        # Lets think about using t later. Paper uses positional embeddings.\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UNet code from https://github.com/milesial/Pytorch-UNet/tree/master\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = (DoubleConv(n_channels, 64))\n",
        "        self.down1 = (Down(64, 128))\n",
        "        self.down2 = (Down(128, 256))\n",
        "        self.down3 = (Down(256, 512))\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = (Down(512, 1024 // factor))\n",
        "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
        "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
        "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
        "        self.up4 = (Up(128, 64, bilinear))\n",
        "        self.outc = (OutConv(64, n_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q49Ks5EuE9Kt"
      },
      "outputs": [],
      "source": [
        "import tonic\n",
        "import torchvision\n",
        "\n",
        "def train_frames(epochs:int=100, diffusion_steps:int=1000, lr=2e-4, device=\"cuda:0\", batch_size=12) -> None:\n",
        "\n",
        "    # ddpm = DDPM(autoencoder_model=AutoEncoderModel(1), betas=(1e-4, 0.02), n_T=diffusion_steps)\n",
        "    ddpm = DDPM(autoencoder_model=UNet(1,1), betas=(1e-4, 0.02), n_T=diffusion_steps)\n",
        "    ddpm.to(device)\n",
        "\n",
        "    transform=torchvision.transforms.Compose([\n",
        "        torch.as_tensor,\n",
        "        lambda x: x.float().unsqueeze(1) / x.max(),\n",
        "        torchvision.transforms.CenterCrop((180, 180)),\n",
        "        torchvision.transforms.Resize((90,90)),\n",
        "        torchvision.transforms.Normalize((0.5,), (0.5)),\n",
        "        ]) # convert to [-1, 1] range\n",
        "\n",
        "    def frame_transform(data):\n",
        "        events, imu, frames = data\n",
        "        frames = transform(frames['frames'])\n",
        "        return frames # events, imu,\n",
        "\n",
        "    # dataset = tonic.datasets.DAVISDATA(save_to=\"data\", recording=[\"shapes_6dof\", \"shapes_translation\" , \"shapes_rotation\"], transform=frame_transform)\n",
        "    # dataset = tonic.datasets.DAVISDATA(save_to=\"data\", recording=[\"slider_close\", \"slider_far\" , \"slider_hdr_close\", \"slider_hdr_far\", \"slider_depth\"], transform=frame_transform)\n",
        "    dataset = tonic.datasets.DAVISDATA(save_to=\"data\", recording=\"all\", transform=frame_transform)\n",
        "    \n",
        "    frames = torch.empty(0, 1, 90, 90)\n",
        "    for imgs, targets in dataset:\n",
        "        frames = torch.cat((frames, imgs))\n",
        "\n",
        "    print(frames.shape)\n",
        "    dataloader = torch.utils.data.DataLoader(frames, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    optim = torch.optim.Adam(ddpm.parameters(), lr=lr)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        ddpm.train()\n",
        "\n",
        "        progress_bar = tqdm(dataloader)\n",
        "        loss_current = None\n",
        "        for x in progress_bar:\n",
        "            optim.zero_grad()\n",
        "            x = x.to(device)\n",
        "            loss = ddpm(x)\n",
        "            loss.backward()\n",
        "            if loss_current is None:\n",
        "                loss_current = loss.item()\n",
        "            else:\n",
        "                loss_current = 0.9 * loss_current + 0.1 * loss.item()\n",
        "            progress_bar.set_description(f\"loss: {loss_current:.4f}\")\n",
        "            optim.step()\n",
        "\n",
        "        ddpm.eval()\n",
        "        with torch.no_grad():\n",
        "            xh = ddpm.sample(4, (1, 90, 90), device)\n",
        "            grid = make_grid(xh, nrow=4)\n",
        "            save_image(grid, f\"./images/ddpm_sample_{i}.png\")\n",
        "            display(Image(f\"./images/ddpm_sample_{i}.png\", width=600, height=600))\n",
        "\n",
        "            # save model\n",
        "            torch.save(ddpm.state_dict(), f\"./ddpm_mnist.pth\")\n",
        "\n",
        "    display(Image(f\"./images/ddpm_sample_{epochs-1}.png\", width=600, height=600))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uuPYW37vFiJP"
      },
      "outputs": [],
      "source": [
        "train_frames(epochs=30, diffusion_steps=1000, batch_size=64) # diffusion_steps aka T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6PgLJS1GxSn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
