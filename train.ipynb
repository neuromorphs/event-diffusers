{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HoDa7tGE_ZF"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from IPython.display import Image\n",
        "from event_diffusion import DDPM, ddpm_schedules, UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7TR3HSi2iZr",
        "outputId": "6f244c31-3536-4701-e1c4-4ea764716563"
      },
      "outputs": [],
      "source": [
        "print(ddpm_schedules(beta1=1e-4, beta2=0.02, T=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q49Ks5EuE9Kt"
      },
      "outputs": [],
      "source": [
        "import tonic\n",
        "import torchvision\n",
        "\n",
        "def train_frames(epochs:int=100, diffusion_steps:int=1000, lr=2e-4, device=\"cuda:0\", batch_size=12) -> None:\n",
        "\n",
        "    # ddpm = DDPM(autoencoder_model=AutoEncoderModel(1), betas=(1e-4, 0.02), n_T=diffusion_steps)\n",
        "    ddpm = DDPM(autoencoder_model=UNet(1,1), betas=(1e-4, 0.02), n_T=diffusion_steps)\n",
        "    ddpm.to(device)\n",
        "\n",
        "    transform=torchvision.transforms.Compose([\n",
        "        torch.as_tensor,\n",
        "        torchvision.transforms.CenterCrop((180, 180)),\n",
        "        torchvision.transforms.Resize((90,90)),\n",
        "        # torchvision.transforms.Normalize((0.5,), (0.5)),\n",
        "        lambda x: x.float().unsqueeze(1) / x.max(),\n",
        "        ]) # convert to [-1, 1] range\n",
        "\n",
        "    def frame_transform(data):\n",
        "        events, imu, frames = data\n",
        "        frames = transform(frames['frames'])\n",
        "        return frames # events, imu,\n",
        "\n",
        "    dataset = tonic.datasets.DAVISDATA(save_to=\"data\", recording=[\"shapes_6dof\", \"shapes_translation\" , \"shapes_rotation\"], transform=frame_transform)\n",
        "    # dataset = tonic.datasets.DAVISDATA(save_to=\"data\", recording=[\"slider_close\", \"slider_far\" , \"slider_hdr_close\", \"slider_hdr_far\", \"slider_depth\"], transform=frame_transform)\n",
        "    # dataset = tonic.datasets.DAVISDATA(save_to=\"data\", recording=\"all\", transform=frame_transform)\n",
        "    \n",
        "    frames = torch.empty(0, 1, 90, 90)\n",
        "    for imgs, targets in dataset:\n",
        "        frames = torch.cat((frames, imgs))\n",
        "\n",
        "    print(frames.shape)\n",
        "    dataloader = torch.utils.data.DataLoader(frames, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    optim = torch.optim.Adam(ddpm.parameters(), lr=lr)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        ddpm.train()\n",
        "\n",
        "        progress_bar = tqdm(dataloader)\n",
        "        loss_current = None\n",
        "        for x in progress_bar:\n",
        "            optim.zero_grad()\n",
        "            x = x.to(device)\n",
        "            loss = ddpm(x)\n",
        "            loss.backward()\n",
        "            if loss_current is None:\n",
        "                loss_current = loss.item()\n",
        "            else:\n",
        "                loss_current = 0.9 * loss_current + 0.1 * loss.item()\n",
        "            progress_bar.set_description(f\"loss: {loss_current:.4f}\")\n",
        "            optim.step()\n",
        "\n",
        "        ddpm.eval()\n",
        "        with torch.no_grad():\n",
        "            xh = ddpm.sample(4, (1, 90, 90), device)\n",
        "            grid = make_grid(xh, nrow=4)\n",
        "            save_image(grid, f\"./images/ddpm_sample_{i}.png\")\n",
        "            display(Image(f\"./images/ddpm_sample_{i}.png\", width=600, height=600))\n",
        "\n",
        "            # save model\n",
        "            torch.save(ddpm.state_dict(), f\"./ddpm_mnist.pth\")\n",
        "\n",
        "    display(Image(f\"./images/ddpm_sample_{epochs-1}.png\", width=600, height=600))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uuPYW37vFiJP"
      },
      "outputs": [],
      "source": [
        "train_frames(epochs=30, diffusion_steps=500, batch_size=64) # diffusion_steps aka T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6PgLJS1GxSn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
