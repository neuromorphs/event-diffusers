{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9HoDa7tGE_ZF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Extremely Minimalistic Implementation of DDPM\n",
        "https://arxiv.org/abs/2006.11239\n",
        "Everything is self contained. (Except for pytorch and torchvision... of course)\n",
        "\"\"\"\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from IPython.display import Image\n",
        "from event_diffusion import DDPM, ddpm_schedules, UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7TR3HSi2iZr",
        "outputId": "6f244c31-3536-4701-e1c4-4ea764716563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alpha_t': tensor([0.9999, 0.9979, 0.9959, 0.9939, 0.9919, 0.9900, 0.9880, 0.9860, 0.9840,\n",
            "        0.9820, 0.9800]), 'sqrt_beta_t': tensor([0.0100, 0.0457, 0.0639, 0.0779, 0.0898, 0.1002, 0.1097, 0.1184, 0.1266,\n",
            "        0.1342, 0.1414]), 'alphabar_t': tensor([0.9999, 0.9978, 0.9937, 0.9877, 0.9797, 0.9699, 0.9582, 0.9448, 0.9296,\n",
            "        0.9129, 0.8946])}\n"
          ]
        }
      ],
      "source": [
        "print(ddpm_schedules(beta1=1e-4, beta2=0.02, T=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q49Ks5EuE9Kt"
      },
      "outputs": [],
      "source": [
        "import tonic\n",
        "import torchvision\n",
        "\n",
        "def train_frames(epochs:int=100, diffusion_steps:int=1000, lr=2e-4, device=\"cuda:0\", batch_size=12) -> None:\n",
        "\n",
        "    # ddpm = DDPM(autoencoder_model=AutoEncoderModel(1), betas=(1e-4, 0.02), n_T=diffusion_steps)\n",
        "    ddpm = DDPM(autoencoder_model=UNet(1,1), betas=(1e-4, 0.02), n_T=diffusion_steps)\n",
        "    ddpm.to(device)\n",
        "\n",
        "    transform=torchvision.transforms.Compose([\n",
        "        torch.as_tensor,\n",
        "        lambda x: x.float().unsqueeze(1) / x.max(),\n",
        "        torchvision.transforms.CenterCrop((180, 180)),\n",
        "        torchvision.transforms.Resize((90,90)),\n",
        "        torchvision.transforms.Normalize((0.5,), (0.5)),\n",
        "        ]) # convert to [-1, 1] range\n",
        "\n",
        "    def frame_transform(data):\n",
        "        events, imu, frames = data\n",
        "        frames = transform(frames['frames'])\n",
        "        return frames # events, imu,\n",
        "\n",
        "    # dataset = tonic.datasets.DAVISDATA(save_to=\"data\", recording=[\"shapes_6dof\", \"shapes_translation\" , \"shapes_rotation\"], transform=frame_transform)\n",
        "    # dataset = tonic.datasets.DAVISDATA(save_to=\"data\", recording=[\"slider_close\", \"slider_far\" , \"slider_hdr_close\", \"slider_hdr_far\", \"slider_depth\"], transform=frame_transform)\n",
        "    dataset = tonic.datasets.DAVISDATA(save_to=\"data\", recording=\"all\", transform=frame_transform)\n",
        "    \n",
        "    frames = torch.empty(0, 1, 90, 90)\n",
        "    for imgs, targets in dataset:\n",
        "        frames = torch.cat((frames, imgs))\n",
        "\n",
        "    print(frames.shape)\n",
        "    dataloader = torch.utils.data.DataLoader(frames, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    optim = torch.optim.Adam(ddpm.parameters(), lr=lr)\n",
        "\n",
        "    for i in range(epochs):\n",
        "        ddpm.train()\n",
        "\n",
        "        progress_bar = tqdm(dataloader)\n",
        "        loss_current = None\n",
        "        for x in progress_bar:\n",
        "            optim.zero_grad()\n",
        "            x = x.to(device)\n",
        "            loss = ddpm(x)\n",
        "            loss.backward()\n",
        "            if loss_current is None:\n",
        "                loss_current = loss.item()\n",
        "            else:\n",
        "                loss_current = 0.9 * loss_current + 0.1 * loss.item()\n",
        "            progress_bar.set_description(f\"loss: {loss_current:.4f}\")\n",
        "            optim.step()\n",
        "\n",
        "        ddpm.eval()\n",
        "        with torch.no_grad():\n",
        "            xh = ddpm.sample(4, (1, 90, 90), device)\n",
        "            grid = make_grid(xh, nrow=4)\n",
        "            save_image(grid, f\"./images/ddpm_sample_{i}.png\")\n",
        "            display(Image(f\"./images/ddpm_sample_{i}.png\", width=600, height=600))\n",
        "\n",
        "            # save model\n",
        "            torch.save(ddpm.state_dict(), f\"./ddpm_mnist.pth\")\n",
        "\n",
        "    display(Image(f\"./images/ddpm_sample_{epochs-1}.png\", width=600, height=600))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uuPYW37vFiJP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([27259, 1, 90, 90])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/426 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "forward() takes 2 positional arguments but 3 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_frames(epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, diffusion_steps\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m)\n",
            "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mtrain_frames\u001b[0;34m(epochs, diffusion_steps, lr, device, batch_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     43\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 44\u001b[0m loss \u001b[39m=\u001b[39m ddpm(x)\n\u001b[1;32m     45\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m loss_current \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Development/event-diffusers/event_diffusion/model.py:50\u001b[0m, in \u001b[0;36mDDPM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m eps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(x)  \u001b[39m# eps ~ N(0, 1)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m x_t \u001b[39m=\u001b[39m (\n\u001b[1;32m     46\u001b[0m     torch\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphabar_t[t, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]) \u001b[39m*\u001b[39m x\n\u001b[1;32m     47\u001b[0m     \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msqrt(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphabar_t[t, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]) \u001b[39m*\u001b[39m eps\n\u001b[1;32m     48\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(eps, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautoencoder_model(x_t, t \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_T))\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
          ]
        }
      ],
      "source": [
        "train_frames(epochs=30, diffusion_steps=1000, batch_size=64) # diffusion_steps aka T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6PgLJS1GxSn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
